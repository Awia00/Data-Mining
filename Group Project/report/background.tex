% !TeX root = report.tex
% !TeX spellcheck = en_GB

\section{Background}
\subsection{Graphs}
A \textit{graph} $G(V,E)$ is data-structure which consists of a set of \textit{vertices} $V$ and a set of \textit{edges} $E$ where an edge is a pair of vertices $(u,v)$. An \textit{undirected graph} implies that if edge $ (u,v) $ exists then $ (v,u) $ exists in the graph. An \textit{unweighted graph} every edge is equal in the cost to traverse it.
Two vertices are \textit{adjacent} if they are part of the same edge $ e $. A \textit{path} $ P $ is a ordered set of edges such that every consecutive edge shares a vertex. To vertices $ u $, $ v $ are \textit{connected} if there exist a path between $ u $, $ v $.

\newpar In an undirected unweighted graph the length of a path P is $ |P| $. A shortest path between two vertices  $ u $, $ v $ is a path $ P $ connecting  $ u $, $ v $ of the smallest length. The Breadth First Search (BFS) algorithm can find the shortest path in time $ \mathcal{O}(n+m) $ where $ n=|V| $ and $ m=|E| $. To be able to solve this combinatorial problem for which the exhaustive search space is $ n! $ the BFS algorithm uses a queue to store previously visited vertices and a map of the vertices and which vertex would come before it in the shortest path.

\todo{why is memory neccesary or maybe further in}

\subsection{Neural Networks}
A \textit{neural network} is an approximation method for \textit{n} dimensional functions. Neural networks consists of a collection of \textit{layers} $L_1 .. L_n$ where $L_1$ is the \textit{input layer}, $L_2 .. L_{n-1}$ are \textit{hidden layers}, and $L_n$ is the \textit{output layer}. Each layer $ Li $ consist of a number of \textit{neurons}. A neuron is connected to other neurons in the layer before and after its own layer.\todo{skal vi nævne at man også kan forbindes til sig selv?} Neurons can send signals to other neurons it is connected to. The strength of a signal is specified by a \textit{signal function} of a neuron. Neurons gets \textit{activated} and sends a signal if itself has become activated a \textit{threshold} amount by other neurons. The signal function is based on a \textit{formula}, a \textit{weight} and a \textit{bias}. 

\newpar The \textit{feed forward algorithm} activates the neurons of the input layer based on some input data, whom in return activates the neurons of the next layer and so on. The values of the output layer is then read. The \textit{backpropagation algorithm} calculates the error of the output neurons compared to the input data and corrects it based on a function. The errors are then backpropagated through the layers. By running the feed forward and backpropagation algorithms in combination with enough data, almost any function can be approximated.

\newpar A \textit{one hot encoding} of a nominal data point, is a conversion of the data point into a bit array $A$ with the length $n$, where $n$ is the number of possible nominal values. Each of the nominal values are then assigned a label $i \in \{0 .. n-1\}$. For a nominal value with label $i$ only $ A[i] $ is $1$. All other elements in $A$ is $0$.

\subsection{Evolutionary Neural Networks}
Evolutionary neural networks is a method of training and developing the topology of neural networks based on evolutionary algorithms. These networks can be trained without using supervised learning but reward based learning where a fitness function describes how well the network does in the specific instance. 

\subsubsection{NeuroEvolution of Augmenting Topologies (NEAT)}
Whereas most other Evolutionary Neural Network algorithms, start from a randomly seeded network, NEAT evolves the topology from the minimal network, where no hidden neurons exist.

\newpar NEAT also uses a technique called innovation numbering, in order to distinguish or compare genes between phenotypes. The technique is used both when mating, that is, pairing two phenotypes in order to produce new outspring, and when "clustering" the phenotypes into species.

\newpar The notion of species is used to protect innovative but untrained structures in a phenotypes. The idea is that, when a new structure has been evolved, it might not have good weights associated, which means that it will initially perform much worse than older and trained phenotypes without this structure. Instead of making all phenotypes compete against each other, the phenotypes are divided into species, that can then compete against each other. This allows the innovative mutations to last a few generations, so they can prove (or disprove) their worth.

\newpar Further details about the NEAT algorithm can be found in \cite{stanley2002evolving}.

\subsection{Evolvable Neural Turing Machine}
In \cite{greve2016evolving} Greve et. al. introduces the Evolvable Neural Turing Machine. A Neural Turing Machine is able to write and read of a non-fixed size memory bank like a regular turing machine. Greve et. al. shows that the evolvable neural turing machine are able to solve simple algorithm tasks such as the copy task, and the continuous T-maze task. 
