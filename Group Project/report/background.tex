% !TeX root = report.tex
% !TeX spellcheck = en_GB

\section{Background}
\subsection{Graphs}
A \textit{graph} $G(V,E)$ is data-structure which consists of a set of \textit{vertices} $V$ and a set of \textit{edges} $E$ where an edge is a pair of vertices $(u,v)$. An \textit{undirected graph} implies that if edge $ (u,v) $ exists then $ (v,u) $ exists in the graph. An \textit{unweighted graph} every edge is equal in the cost to traverse it.
Two vertices are \textit{adjacent} if they are part of the same edge $ e $. A \textit{path} $ P $ is a ordered set of edges such that every consecutive edge shares a vertex. To vertices $ u $, $ v $ are \textit{connected} if there exist a path between $ u $, $ v $.

\newpar In an undirected unweighted graph the length of a path P is $|P|$. A shortest path between two vertices  $u$, $v$ is a path $P$ connecting  $u$, $v$ of the smallest length. The Breadth First Search (BFS) algorithm can find the shortest path in time $\mathcal{O}(n+m)$ where $n=|V|$ and $m=|E|$. To be able to solve this combinatorial problem for which the exhaustive search space is $n!$ the BFS algorithm uses a queue to store previously visited vertices and a map of the vertices and which vertex would come before it in the shortest path.

\subsection{Neural Networks}
A \textit{neural network} is an approximation method for \textit{n} dimensional functions. Neural networks consists of a collection of \textit{layers} $L_1 .. L_n$ where $L_1$ is the \textit{input layer}, $L_2 .. L_{n-1}$ are \textit{hidden layers}, and $L_n$ is the \textit{output layer}. Each layer $L_i$ consist of a number of \textit{neurons}. A neuron is connected to other neurons in the layer before and after its own layer. Neurons can send signals to other neurons it is connected to. The strength of a signal is specified by a \textit{signal function} of a neuron. Neurons gets \textit{activated} and sends a signal if itself has become activated a \textit{threshold} amount by other neurons. The signal function is based on a \textit{formula}, a \textit{weight} and a \textit{bias}. 

\newpar The \textit{feed forward algorithm} activates the neurons of the input layer based on some input data, whom in return activates the neurons of the next layer and so on. The values of the output layer is then read. The \textit{back-propagation algorithm} calculates the error of the output neurons compared to the input data and corrects it based on a function. The errors are then back-propagated through the layers. By running the feed forward and back-propagation algorithms in combination with enough data, almost any function can be approximated.

\newpar It is also possible for a neural network to be recurrent. These kinds of networks allow a neuron to be connected to itself, other neurons in the same layer, or maybe the neurons of the entire network, depending on the configuration.

\newpar A \textit{one hot encoding} of a nominal data point, is a conversion of the data point into a bit array $A$ with the length $n$, where $n$ is the number of possible nominal values. Each of the nominal values are then assigned a label $i \in \{0 .. n-1\}$. For a nominal value with label $i$ only $ A[i] $ is $1$. All other elements in $A$ is $0$. This encoding is often used for neural networks, as each element of $A$ can be used as the input for a single neuron in the input layer of a neural network.

\subsection{Evolutionary Neural Networks}
Evolutionary neural networks is a method of training the weights of a neural network based on evolutionary algorithms. These networks can be trained without using supervised learning but reward based learning where a fitness function describes how well the network does in the specific instance. The use of evolutionary algorithms means that high performing neural networks are the base for the next generations, rather than choosing arbitrary or completely random networks in each generation.

\subsubsection{NeuroEvolution of Augmenting Topologies (NEAT)}
Whereas most other Evolutionary Neural Network algorithms, start from a randomly seeded network with a fixed topology, NEAT evolves the topology from the minimal network, where no hidden neurons exist.

\newpar NEAT has a few main components, that is used to improve over previous neuroevolution algorithms:

\begin{itemize}
	\item Innovation numbering
	\item Speciation
	\item Incremental growth from minimal structure
\end{itemize}

\newpar NEAT uses a technique called innovation numbering, in order to distinguish or compare genes between genomes. The technique is used both when mating, that is, pairing two phenotypes in order to produce new outspring, and when grouping the phenotypes into species.

\newpar The notion of species is used to protect innovative but untrained structures in new genomes. The idea is that, when a new structure has been introduced, it might not have good weights associated, which means that it will initially perform worse than older and trained genomes without this structure. Instead of having all genomes compete against each other, the genomes are divided into species, where the genomes inside a specie compete against each other. This allows the innovative mutations to last some generations, so they can prove, or disprove, their worth.

\newpar Further details about the NEAT algorithm can be found in \cite{stanley2002evolving}.

\subsection{Evolvable Neural Turing Machine (ENTM)}
In \cite{greve2016evolving} Greve et. al. introduces the Evolvable Neural Turing Machine as an extension of the Neural Turing Machine (NTM, \cite{graves2014neural}), with the difference that the ENTM does not require the network to read through the entire memory bank at each time-step, as the NTM does. Furthermore the memory bank can be theoretically unlimited in the ENTM setup.

\newpar With this change comes also a different way of addressing the memory bank. The Turing Machine part of ENTM has four operations that can be performed on the memory banks read/write-head:

\begin{enumerate}
	\item Write - Writes the content of the write vector to the current position in the memory bank.\footnote{This is a simplification. The content is interpolated with the existing vector.}
	\item Content Jump - Moves the read/write head to the memory bank position that is most similar to the write vector.
	\item Shift - The read/write head can be shifted one position to the left or right, or it can be kept at the current position.
	\item Read - The content of the current position is used as input for the next artificial neural network in the next time-step.
\end{enumerate}

\newpar Furthermore the ENTM is trained and evolved using NEAT, whereas the NTM uses gradient descent. This enforces looser requirements to the structure and activation functions of the neural network.

\newpar Greve et. al. shows that the ENTM is able to solve simple algorithm tasks such as the copy task, and the continuous T-maze task.
