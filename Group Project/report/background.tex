% !TeX root = report.tex
% !TeX spellcheck = en_GB

\section{Background}
\subsection{Graphs}
A \textit{graph} $G(V,E)$ is data-structure which consists of a set of \textit{vertices} $V$ and a set of \textit{edges} $E$ where an edge is a pair of vertices $(u,v) $ where $u,v\in V$. An \textit{undirected graph} implies that if edge $ (u,v) $ exists then $ (v,u)$ exists in the graph. In an \textit{unweighted graph} there is no cost distinction between edges.
Two vertices are \textit{adjacent} if they share an edge $ e $. A \textit{path} $ P $ is a ordered set of edges such that for every two consecutive edges $ e $ and $ f $, $ e=(u,v) $, $ f=(v,w) $. To vertices $ u $, $ v $ are \textit{connected} if there exist a path between $ u $ and $ v $. The term \textit{graph} will be used to denote undirected unweighted graphs in the remainder of the report.

\newpar In a graph the length of a path P is $|P|$. A shortest path between two vertices  $u$, $v$ is a path $P$ connecting  $u$, $v$ of minimum length. The Breadth First Search (BFS) algorithm can find the shortest path in time $\mathcal{O}(n+m)$ where $n=|V|$ and $m=|E|$. BFS uses a queue to store vertices in the fringe and for each vertex a pointer to the vertex that comes before it on the shortest path.

\subsection{Neural Networks}
A \textit{neural network} is an approximation method for $ n $ dimensional continuous functions. Neural networks consists of a ordered set of \textit{layers} $L_1 .. L_n$ where $L_1$ is the \textit{input layer}, $L_2 .. L_{n-1}$ are \textit{hidden layers}, and $L_n$ is the \textit{output layer}. Each layer $L_i$ consist of a number of \textit{neurons}. 

\newpar The \textit{topology} of a neural network describes the number of layers, the number of neurons in each layer, and how neurons are connected. In an \textit{acyclic} topology each neuron is connected to other neurons only in the layers before and after its own layer. In a \textit{recurrent} topology neurons are allowed to be connected to itself, other neurons in the same layer, or neurons of any other layer in the network.

\newpar Neurons can send signals to other neurons it is connected to and the strength of a signal is specified by a \textit{signal function}. A neuron is activated if the sum of the signals sent to it reaches a threshold. The signal function is based on a \textit{formula}, a \textit{weight} and a \textit{bias}. 

\newpar A \textit{one hot encoding} of a nominal data point, is a conversion of the data point into a bit array $A$ with the length $n$, where $n$ is the number of possible nominal values. Each of the nominal values are then assigned a label $i \in \{0 .. n-1\}$. For a nominal value with label $i$ only $ A[i] $ is $1$, all other elements in $A$ is $0$. This encoding is often used for neural networks, as each element of $A$ can be used as the input for a single neuron in the input layer.

\subsection{Neuroevolution}
\textit{Neuroevolution} is a method for training the weights and in some cases the topology of a neural network based on evolutionary algorithms. Instead of using supervised learning, neuroevolution uses reward based learning where a fitness function describes how well the network does in a specific instance. 
In contrast to gradient descent which minimizes errors, neuroevolution uses biologically inspired selection methods such as \textit{mutation} and \textit{reproduction} to optimize the fitness of a \textit{population} of networks. The best performing network in a population is declared the \textit{champion}.

\subsubsection{NeuroEvolution of Augmenting Topologies (NEAT)}
NEAT is introduced by Stanley and Miikkulainen in \cite{stanley2002evolving} as a new method for neuroevolution. In NEAT a neural network is called a \textit{phenotype}. NEAT uses the concept of \textit{genes} to both describe neurons, and connections between them. A \textit{genome} consists of genes where some of them might be deactivated. The active genes of a genome corresponds to a phenotype. 

\newpar To improve over previous methods of neuroevolution NEAT uses: Innovation numbering, speciation, and incremental growth from minimal structure.

\newpar Innovation numbering uniquely labels a gene to distinguish or compare genes across genomes. The technique is used both in reproduction and when grouping the genomes into \textit{species}.

\newpar The notion of species is used to protect innovative but unoptimized structures in new genomes. The phenotype of a genome with a newly introduced gene, might perform worse due to unoptimized weights, even if the new gene is useful for further optimization. Instead of having all genomes compete against each other, they are divided into species, where only genomes inside a specie compete against each other. This allows the innovative mutations to last some generations, so they can prove, or disprove, their worth.

\subsection{Evolvable Neural Turing Machine (ENTM)}
The \textit{Neural Turing Machine} (NTM, \cite{graves2014neural}) introduces a fixed size memory bank which allows the network to store and recall information in each activation. This allows the network to describe variables and therefore to approximate algorithms.

\newpar In \cite{greve2016evolving} Greve et. al. introduce the \textit{Evolvable Neural Turing Machine} as an extension of the NTM with two main differences. 

First of all ENTM does not require the network to read through the entire memory bank at each time-step which entails that the memory bank can be theoretically unlimited. Second, ENTM uses NEAT to optimize fitness where the NTM uses gradient descent.

\newpar The ENTM addresses the memory with the following four instructions:

\begin{enumerate}
	\item Write - Writes the content of the write vector to the current position in the memory bank.
	\item Content Jump - Moves the read/write head to the memory bank position that is most similar to the write vector.
	\item Shift - The read/write head can be shifted one position to the left or right, or it can be kept at the current position.
	\item Read - The content of the current position is used as input for the next artificial neural network in the next time-step.
\end{enumerate}

\newpar Greve et. al. show that the ENTM is able to solve simple algorithmic tasks such as the copy task, and the continuous T-maze task.
