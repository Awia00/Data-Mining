% !TeX root = report.tex
% !TeX spellcheck = en_GB

\section{Experiment}
The problem definition is as follows: 
Given a graph $G$, a source vertex $s$, and a goal vertex $t$, find the shortest path from $s$ to $t$ in $G$.

\newpar We have restricted the problem to instances where a path between $s$ and $t$ exists.

\subsection{Instances}
In order to feed the instance into the network, we use an encoding of the adjacency matrix of the graph, where each edge has the value 1, and each non-edge has the value 0. The source and goal vertices are one-hot encoded. See \autoref{fig:input:encoding} for an example. The output of the ENTM is interpreted as a one-hot encoding of the next vertex on the path to the goal. After each step the source vertex is updated to the previously outputted vertex in the new input to the ENTM.

\begin{figure}[ht]
	\centering
	\begin{subfigure}{.5\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figures/encoding.png}
		\subcaption{}
	\end{subfigure}%
	\begin{subfigure}{.5\textwidth}
		\centering
		\begin{tabular}{|c|c|c|c|c|}
			\hline
			&\textbf{1}&\textbf{2}&\textbf{3}&\textbf{4}\\\hline
			\textbf{1}&0&1&0&0\\\hline
			\textbf{2}&1&0&1&1\\\hline
			\textbf{3}&0&1&0&1\\\hline
			\textbf{4}&0&1&1&0\\\hline
		\end{tabular}
		\subcaption{}
	\end{subfigure}\par\bigskip
	\begin{adjustbox}{center}
		\begin{subfigure}{1.3\textwidth}
			\centering
			\begin{tabu}{|c|c|c|c||c|c|c|c||c|c|c|c||c|c|c|c|[2pt]c|c|c|c|[2pt]c|c|c|c|}
				\hline
				0&1&0&0&1&0&1&1&0&1&0&1&0&1&1&0&1&0&0&0&0&0&0&1\\\hline
			\end{tabu}
			\subcaption{}
		\end{subfigure}
	\end{adjustbox}
	\caption{(a) Shows a simple graph, (b) its adjacency matrix and (c) an encoding of the entire instance where $s=1$ and $t=4$. The three structures are the flattened adjacency table, the one-hot encoding of $s$, and the one-hot encoding of $t$}
	\label{fig:input:encoding}
\end{figure}

\newpar The instances are generated in a manner which tries to avoid over fitting and support general solutions. This is done by using randomly generated graphs with certain restrictions. The graph must contain a path of a given length to ensure a certain level of complexity. This level of complexity filters out optimal strategies on non general case graphs such as graphs with paths of length 1 where the optimal solution is outputting the goal vertex. Generating these graphs is done by picking two random vertices $ s $ and $ t $ and randomly pick vertices that forms a path from $ s $ to $ t $. The remaining vertices are then connected to the set of explored vertices randomly. This ensures that the ENTM cannot just follow edges but need to explore branches of the graph. The generated graphs are of fixed size to ensure neural network compatibility.

\subsection{Fitness}
The fitness function is defined as a summation of scores for the output of each step on the shortest path. A function $ DistTo(u) $ returns the distance from vertex $ u $ to the goal vertex. The vertex $ current $ describes the vertex which was given to the network and $ next $ denotes the networks choice of next vertex. The following scores are given in each step:

\begin{itemize}
	\item[] 1 point: if $ DistTo(next) > DistTo(current) $
	\item[] 2 points: if $ DistTo(next) = DistTo(current) $
	\item[] 4 points: if $ DistTo(next) < DistTo(current) $
\end{itemize}

\noindent In the case that one step resulted in a move where no edge exists the overall score is set to 0. No further steps are performed.

\newpar The motivation these scores is to reward neural networks that understand the underlying graph problem more than a network that chooses arbitrarily.

\newpar The final score is normalized by the following formula $ \frac{score}{DistTo(source)*4} $ where the denominator is the  maximum score.

\subsection{The Experiments}
To assess and measure how well the shortest path problem is solved, four different types of experiments are performed. The first experiment uses the NEAT framework to train and build neural networks with recurrent connections. The next experiment uses the ENTM extension of the NEAT framework. These experiments are still using recurrent neural networks. The memory is unlimited (within the bounds of the host machine), the write vector size is 10 and the default jump mechanism introduced in \cite{luders2017continual} is used. The two experiments are repeated on acyclic topologies.

\newpar All experiments uses the exact same configuration for NEAT and runs on the same instances to make it easier to compare them. Each configuration uses a population of 500 divided into 30 species. Each phenotype is tested on 25 unique graphs per generation. An example configuration can be seen in \autoref{appendix:sharpneat:configurations}. The appendix also contains the changes between the different experiments.