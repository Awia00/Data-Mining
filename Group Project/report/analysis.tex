% !TeX root = report.tex
% !TeX spellcheck = en_GB

\section{Analysis}
From the results above it seems that the structure of our input does not allow for good results.

\newpar Recurrent and acyclic neural networks as well as Turing enabled and disabled networks never (in 10.000 generations) achieve higher fitness than approximately 0.35. To put the value into perspective the result is compared to three suboptimal strategies.\footnote{We have included an Excel sheet to the submission of the report, where the calculations can be seen. The file is named Strategies.xlsx.} A strategy where moves are picked randomly including non-edges will result in an average fitness of 0.000126. A strategy which randomly picks legal moves will get an average fitness of 0.67 and a strategy which always moves away from the goal will get a fitness of 0.25. The span between these numbers are great, and it is clear that as soon as a strategy learns to not pick non-edges the fitness greatly increases. The observation is therefore that the champion must have some understanding of the graph but only enough so that it most of the time does not pick non-edges. The strategy of always moving away from the goal is in some sense just as hard a problem as the shortest path to the goal, therefore it is not likely that the champion uses this strategy. 

To further analyse the champions we examine the behaviour of the champion on a collection of graphs. The champions seem to pick a lot of non edges but in a lot of instances the weights of the output contains high values for multiple decisions. In \autoref{table:analysis:1} an action of one of the champions with memory can be seen. It decides to move to vertex 3 which is faulty and results in score 0 on the entire iteration. But it is interesting to see that the only valid move of the instance also has a high score which only differs in the 5th decimal. This supports the indication that the network in general has higher chances of picking the correct moves than a totally uniform random strategy.

\begin{table}
	\centering
	\begin{tabular}{|l|l|l|}
		\hline
		Move:&	decision & score for decision\\
		\hline
		0:&	0.5 & 0 \\
		\hline
		1:&	0.5	& 0\\
		\hline
		2:&	0.5	& 0\\
		\hline
		3:&	0.99999 & 0\\
		\hline
		4:&	0.5	& 0\\
		\hline
		5:&	0.5	& 0\\
		\hline
		6:&	0.99392 & 0\\
		\hline
		7:&	0.99994 & 4\\
		\hline
		8:&	0.5 & 0	\\
		\hline
		9:&	0.5 & 0\\
		\hline
	\end{tabular}
	\caption{The network is on vertex 1 goal is 0. Should move to 7 as it is the only possible edge. Chooses to move to 3 which is connected to 0.}
	\label{table:analysis:1}
\end{table}

\newpar One possible reason for this, might be that the encoding of the problem is not feasible for the ENTM. If we compare our encoding to that of the Copy Task from \cite{greve2016evolving}, they made sure to signal the network when starting a new assignment and when asking it to return the content back. Also, they supplied the input in multiple time-steps rather than giving the entire sequence all at once.

\newpar The shortest path problem differ from the Copy task in that the memory bank should not only be used to store content as provided. It should be used to model some structure that would allow for the planning problem to be performed. Since we ask for output in every iteration (also the first), we don't allow the ENTM to preprocess anything. This might be another place to tweak the problem encoding. Further experiments might try to feed the graph in multiple steps and different encodings, and train the network to signal when it is ready to answer on the shortest path problem.